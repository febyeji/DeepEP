// SPDX-License-Identifier: MIT 
// SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.

#include "executor.cuh"
#include <vector>
#include <cstdint>

Executor::Executor(int local_rank, int node_rank, std::string base_path, std::string comm_id, bool load_cached_kernels, bool enable_custom_allgather) : local_rank(local_rank), node_rank(node_rank), kernel_cache(node_rank, local_rank, base_path, comm_id, load_cached_kernels), enable_custom_allgather(enable_custom_allgather) {}  

void Executor::set_intra_node_buffers(IntraNodeDispatchBuffers *intra_node_dispatch_buffers, IntraNodeCombineBuffers *intra_node_combine_buffers) {
    this->intra_node_dispatch_buffers = intra_node_dispatch_buffers;
    this->intra_node_combine_buffers = intra_node_combine_buffers;
}

#ifdef HYBRID_EP_BUILD_MULTINODE_ENABLE
void Executor::set_inter_node_buffers(InterNodeDispatchBuffers *inter_node_dispatch_buffers, InterNodeCombineBuffers *inter_node_combine_buffers) {
    this->inter_node_dispatch_buffers = inter_node_dispatch_buffers;
    this->inter_node_combine_buffers = inter_node_combine_buffers;
}
#endif

torch::Tensor Executor::allgather_routing_map(
    CustomAllgather &allgather_obj,
    HybridEpConfigInstance config,
    torch::Tensor local_routing_map,
    py::object process_group
){
    nvtxRangePushA("allgather_routing_map in hybrid-ep");

    auto torch_distributed = py::module_::import("torch.distributed");
    auto num_of_expert = local_routing_map.size(-1);
    auto num_of_tokens_per_rank = local_routing_map.size(-2);
    auto group_size = process_group.attr("size")().cast<int>();
    assert(num_of_expert == config.num_of_experts_per_rank * config.num_of_ranks_per_node * config.num_of_nodes);

    torch::Tensor global_routing_map;
    // At inter-node case, we will use NCCL allgather
    if(config.num_of_nodes > 1 || !enable_custom_allgather) {
        global_routing_map = torch::empty(
            {num_of_tokens_per_rank * group_size, num_of_expert},
            torch::TensorOptions().dtype(torch::kBool).device(torch::kCUDA)
        );
        torch_distributed.attr("all_gather_into_tensor")(global_routing_map, local_routing_map, process_group);
    } else { // At intra-node case, we will use custom allgather
        allgather_obj.launch(local_routing_map, /*NUM_OF_SMS=*/32, at::cuda::getCurrentCUDAStream());
        global_routing_map = torch::from_blob(
            allgather_obj.get_output_buffer(), 
            {num_of_tokens_per_rank * group_size, num_of_expert},
            torch::TensorOptions().dtype(torch::kBool).device(torch::kCUDA)
        );
    }

    nvtxRangePop();  // End of allgather_routing_map nvtx range
    return global_routing_map;
}

std::tuple<torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor>
Executor::metadata_preprocess_core(
    HybridEpConfigInstance config, 
    hybrid_ep::tmp_state_t *preprocessing_tmp,
    torch::Tensor global_routing_map,
    int num_of_tokens_per_rank,
    bool non_blocking
) {
  nvtxRangePushA("metadata_preprocess_core in hybrid-ep");

  // TMA requires (remainder_chunk_size * num_of_ranks_per_node * 4) % 16 == 0
  const int remainder_chunk_size = num_of_tokens_per_rank % config.num_of_tokens_per_chunk_dispatch_api;
  if (remainder_chunk_size != 0) {
    const int tma_load_size = remainder_chunk_size * config.num_of_ranks_per_node * sizeof(int32_t);
    TORCH_CHECK(
        tma_load_size % 16 == 0,
        "TMA 16B alignment error: tma_load_size = remainder_chunk(", remainder_chunk_size,
        ") * ranks_per_node(", config.num_of_ranks_per_node, ") * 4 = ", tma_load_size, 
        "B, must be multiple of 16B."
    );
  }

  // padding for the routing map
  const int rdma_to_attn_map_size_per_node = (((num_of_tokens_per_rank - 1) / 16) + 1) * 16;

  // Construt the output tensor of the metadata preprocessing kernel.
  auto sparse_to_dense_map =
      torch::empty({num_of_tokens_per_rank * config.num_of_nodes,
                    config.num_of_ranks_per_node},
                   torch::dtype(torch::kInt32).device(torch::kCUDA));
  auto rdma_to_attn_map =
      torch::empty({rdma_to_attn_map_size_per_node, config.num_of_nodes},
                   torch::dtype(torch::kBool).device(torch::kCUDA));
  auto attn_to_rdma_map =
      torch::empty({num_of_tokens_per_rank, config.num_of_nodes - 1},
                   torch::dtype(torch::kBool).device(torch::kCUDA));
  torch::Tensor num_of_tokens_for_experts;
  if (non_blocking) {
    num_of_tokens_for_experts =
        torch::empty({1}, torch::dtype(torch::kInt32).device(torch::kCUDA));
  } else {
    num_of_tokens_for_experts =
        torch::empty({1}, torch::dtype(torch::kInt32).pinned_memory(true));
  }
  auto local_expert_routing_map = torch::empty(
      {num_of_tokens_per_rank * config.num_of_ranks_per_node * config.num_of_nodes, config.num_of_experts_per_rank},
      torch::dtype(torch::kBool).device(torch::kCUDA));
  
  kernel_cache.run_proprecess_kernel(
      config, global_routing_map.data_ptr<bool>(), preprocessing_tmp,
      sparse_to_dense_map.data_ptr<int32_t>(),
      rdma_to_attn_map.data_ptr<bool>(), attn_to_rdma_map.data_ptr<bool>(),
      num_of_tokens_for_experts.data_ptr<int32_t>(),
      local_expert_routing_map.data_ptr<bool>(), static_cast<int>(node_rank),
      static_cast<int>(local_rank), num_of_tokens_per_rank, at::cuda::getCurrentCUDAStream());

  nvtxRangePop();  // End of metadata_preprocess_core nvtx range
  return std::make_tuple(sparse_to_dense_map, rdma_to_attn_map, attn_to_rdma_map, num_of_tokens_for_experts, local_expert_routing_map);
}

std::tuple<torch::Tensor, torch::Tensor, torch::Tensor> 
Executor::dispatch_preprocess(HybridEpConfigInstance config, DispatchArgs& args) {
    nvtxRangePushA("dispatch_preprocess in hybrid-ep");
    if(config.num_of_nodes > 1) {
#ifdef HYBRID_EP_BUILD_MULTINODE_ENABLE
        auto sizeof_token_data_type = get_token_data_type_size(config.token_data_type);
        CUDA_CHECK(cudaMemcpyAsync(inter_node_dispatch_buffers->attn_input_token, args.hidden.data_ptr(), args.hidden.numel() * sizeof_token_data_type, cudaMemcpyDeviceToDevice, args.stream));
        if(config.forward_dispatch_api) {
            CUDA_CHECK(cudaMemcpyAsync(inter_node_dispatch_buffers->attn_input_prob, args.probs.data_ptr(), args.probs.numel() * sizeof(float), cudaMemcpyDeviceToDevice, args.stream));
        }
        if(config.token_data_type == APP_TOKEN_DATA_TYPE::UINT8) {
            CUDA_CHECK(cudaMemcpyAsync(inter_node_dispatch_buffers->attn_input_scaling_factor, args.scaling_factor.data_ptr(), args.scaling_factor.numel() * sizeof(float), cudaMemcpyDeviceToDevice, args.stream));
        }
#else
        throw std::runtime_error("Multi-node support is not enabled in this build.");
#endif
    } 

    torch::Tensor row_id_map;
    torch::Tensor tokens_per_expert;
    torch::Tensor overflow_flag;

    if(args.enable_permute) {
        if(args.row_id_map.has_value()){
            assert(args.num_permuted_tokens >= 0);
            row_id_map = args.row_id_map.value();
        } else {
            assert(args.local_expert_routing_map.has_value());
            std::tie(row_id_map, tokens_per_expert, overflow_flag) = permute_preprocessing(
                args.local_expert_routing_map.value().data_ptr<bool>(), 
                args.num_dispatched_tokens_tensor.value(),
                args.max_num_dispatched_tokens, 
                config.num_of_experts_per_rank, 
                args.pad_multiple, 
                config.num_of_blocks_preprocessing_api,
                args.num_permuted_tokens,
                args.non_blocking,
                args.stream
            );
            args.row_id_map = row_id_map;

            // If we want to put the tokens_per_expert/num_dispatched_tokens_tensor can be used in the host, we need to synchronize the stream.
            if (!args.non_blocking) {
                cudaStreamSynchronize(args.stream);
                if (args.num_permuted_tokens < 0) {
                    const int64_t* tokens_per_expert_ptr = tokens_per_expert.data_ptr<int64_t>();
                    int64_t num_permuted_tokens = 0;
                    for (int i = 0; i < config.num_of_experts_per_rank; ++i) {
                        num_permuted_tokens += static_cast<int64_t>(tokens_per_expert_ptr[i]);
                    }
                    args.num_permuted_tokens = num_permuted_tokens;
                }
            }
        }
    }
    nvtxRangePop();  // End of dispatch_preprocess nvtx range

    return std::make_tuple(row_id_map, tokens_per_expert, overflow_flag);
}

template void Executor::dispatch_core<uint8_t>(HybridEpConfigInstance config, DispatchArgs& args);
template void Executor::dispatch_core<uint16_t>(HybridEpConfigInstance config, DispatchArgs& args);

template<typename DType>
void Executor::dispatch_core(HybridEpConfigInstance config, DispatchArgs& args) {
    nvtxRangePushA("dispatch_core in hybrid-ep");

    hybrid_ep::dispatch_kernel_param_t<DType> param;
    // Setup input pointers
    if(config.num_of_nodes > 1) {
#ifdef HYBRID_EP_BUILD_MULTINODE_ENABLE
        param.attn_input_token = reinterpret_cast<DType*>(inter_node_dispatch_buffers->attn_input_token);
        param.attn_input_prob = reinterpret_cast<float*>(inter_node_dispatch_buffers->attn_input_prob);
        param.attn_input_token_scaling_factor = reinterpret_cast<float*>(inter_node_dispatch_buffers->attn_input_scaling_factor);
#else
        throw std::runtime_error("Multi-node support is not enabled in this build.");
#endif
    } else {
        param.attn_input_token = reinterpret_cast<DType*>(args.hidden.data_ptr());
        param.attn_input_prob = (config.forward_dispatch_api) ? 
            reinterpret_cast<float*>(args.probs.data_ptr()) : nullptr;
        param.attn_input_token_scaling_factor = (config.token_data_type == APP_TOKEN_DATA_TYPE::UINT8) ? 
            reinterpret_cast<float*>(args.scaling_factor.data_ptr()) : nullptr;
    }
    
    // Setup output pointers
    for (int i = 0; i < config.num_of_ranks_per_node; i++) {
      param.expert_output_token[i] = reinterpret_cast<DType*>(
          intra_node_dispatch_buffers->expert_output_token_all_ranks[i]);
      param.expert_output_prob[i] = intra_node_dispatch_buffers->expert_output_prob_all_ranks[i];
      param.expert_output_scaling_factor[i] = 
          intra_node_dispatch_buffers->expert_output_scaling_factor_all_ranks[i];
    }
    
    // Setup local buffer pointers
#ifdef HYBRID_EP_BUILD_MULTINODE_ENABLE
    param.rdma_inter_node_group_token = reinterpret_cast<DType*>(
        inter_node_dispatch_buffers->rdma_inter_node_group_token);
    param.rdma_inter_node_group_prob = inter_node_dispatch_buffers->rdma_inter_node_group_prob;
    param.rdma_inter_node_group_scaling_factor = 
        inter_node_dispatch_buffers->rdma_inter_node_group_scaling_factor;
    param.rdma_inter_node_group_flags = inter_node_dispatch_buffers->rdma_inter_node_group_flags;
#endif
    param.intra_node_write_completion_flags = 
        intra_node_dispatch_buffers->intra_node_write_completion_flags;
    param.rdma_to_attn_map = args.rdma_to_attn_map.data_ptr<bool>();
    param.attn_to_rdma_map = args.attn_to_rdma_map.data_ptr<bool>();
    param.sparse_to_dense_map = args.sparse_to_dense_map.data_ptr<int32_t>();

    // Misc
    param.local_rank = local_rank;
    param.node_rank = node_rank;
    param.num_of_tokens_per_rank = args.num_of_tokens_per_rank;
    param.expected_intra_node_flag_value = intra_node_dispatch_buffers->expected_intra_node_flag_value;
#ifdef HYBRID_EP_BUILD_MULTINODE_ENABLE
    param.expected_rdma_flag_value = inter_node_dispatch_buffers->expected_rdma_flag_value;
    param.d_qps_gpu = reinterpret_cast<void **>(inter_node_dispatch_buffers->d_qps_gpu);
    param.mr_info = reinterpret_cast<void*>(inter_node_dispatch_buffers->mr_info);
#endif
    
    // Launch kernel
    kernel_cache.run_dispatch_kernel<DType>(config, param, args.stream);
    nvtxRangePop();  // End of dispatch_core nvtx range
}

std::tuple<torch::Tensor, c10::optional<torch::Tensor>, c10::optional<torch::Tensor> >
Executor::dispatch_postprocess(HybridEpConfigInstance config, DispatchArgs& args) {
    nvtxRangePushA("dispatch_postprocess in hybrid-ep");

    // Create and return output tensors
    // The output tensor of the dispatch kernel.
    torch::Tensor dispatched_tokens;
    c10::optional<torch::Tensor> dispatched_probs;
    c10::optional<torch::Tensor> dispatched_scaling_factor;

    if(args.enable_permute) {
        // Use permute kernel to avoid standalone D2D memory copy
        assert(args.num_dispatched_tokens_tensor.has_value());
        assert(args.row_id_map.has_value());
        assert(args.num_permuted_tokens >= 0);
    
        // Prepare the arguments for the permute kernel
        PermuteArgs permute_args;
        permute_args.tokens_ptr = reinterpret_cast<void*>(intra_node_dispatch_buffers->expert_output_token);
        permute_args.probs_ptr = reinterpret_cast<float*>(intra_node_dispatch_buffers->expert_output_prob);
        permute_args.scaling_factor_ptr = reinterpret_cast<float*>(intra_node_dispatch_buffers->expert_output_scaling_factor);
        permute_args.row_id_map = args.row_id_map.value();
        permute_args.hidden_size = config.hidden_dim;
        permute_args.scales_per_token = config.hidden_dim / 128;
        permute_args.num_dispatched_token_tensor = args.num_dispatched_tokens_tensor.value();
        permute_args.num_permuted_token = args.num_permuted_tokens;
        permute_args.num_ranks_per_node = config.num_of_ranks_per_node;
        permute_args.num_of_local_experts = config.num_of_experts_per_rank;
        permute_args.pad_multiple = args.pad_multiple;
        permute_args.local_rank = local_rank;
        permute_args.use_fp8 = config.token_data_type == APP_TOKEN_DATA_TYPE::UINT8;
        permute_args.with_probs = config.forward_dispatch_api;
        permute_args.token_options = args.hidden.options();
        permute_args.stream = args.stream;
        permute_args.num_of_blocks_permute_api = config.num_of_blocks_permute_api;
        
        if(config.token_data_type == APP_TOKEN_DATA_TYPE::UINT16) {
            std::tie(dispatched_tokens, dispatched_scaling_factor, dispatched_probs) = 
                permute_launcher<uint16_t, float, float>(permute_args);
        } else if (config.token_data_type == APP_TOKEN_DATA_TYPE::UINT8) {
            std::tie(dispatched_tokens, dispatched_scaling_factor, dispatched_probs) = 
                permute_launcher<uint8_t, float, float>(permute_args);
        }else {
            throw std::runtime_error("Unsupported token data type: " + type_to_string(config.token_data_type));
        }
    }else {
        // D2D copy the result to the pytorch tensor
        int num_dispatched_tokens = 0;
        if (args.num_dispatched_tokens >= 0) {
          num_dispatched_tokens = args.num_dispatched_tokens;
        } else {
          num_dispatched_tokens = args.num_dispatched_tokens_tensor.value().item<int>();
        }
        size_t sizeof_token_data_type = get_token_data_type_size(intra_node_dispatch_buffers->data_type);
        dispatched_tokens = torch::empty(
            {num_dispatched_tokens, config.hidden_dim}, 
            torch::dtype(args.hidden.dtype()).device(torch::kCUDA)
        );
        auto res_sz = static_cast<size_t>(num_dispatched_tokens) * config.hidden_dim * sizeof_token_data_type;
        CUDA_CHECK(cudaMemcpyAsync(dispatched_tokens.data_ptr(), intra_node_dispatch_buffers->expert_output_token, res_sz, cudaMemcpyDeviceToDevice, args.stream));

        if(config.forward_dispatch_api) {
            dispatched_probs = torch::empty({num_dispatched_tokens,
                config.num_of_experts_per_rank * config.num_of_ranks_per_node},
                            torch::dtype(torch::kFloat32).device(torch::kCUDA));
            auto probs_sz = static_cast<size_t>(num_dispatched_tokens) * config.num_of_experts_per_rank * config.num_of_ranks_per_node * sizeof(float);
            CUDA_CHECK(cudaMemcpyAsync(dispatched_probs.value().data_ptr<float>(),
                intra_node_dispatch_buffers->expert_output_prob,
                probs_sz, cudaMemcpyDeviceToDevice, args.stream));
        }

        if(config.token_data_type == APP_TOKEN_DATA_TYPE::UINT8) {
            dispatched_scaling_factor = torch::empty({
                    num_dispatched_tokens, 
                    config.hidden_dim / 128}, 
                    torch::dtype(torch::kFloat32).device(torch::kCUDA));
            auto scaling_factor_sz = static_cast<size_t>(num_dispatched_tokens) * config.hidden_dim / 128 * sizeof(float);
            CUDA_CHECK(cudaMemcpyAsync(dispatched_scaling_factor.value().data_ptr<float>(),
                intra_node_dispatch_buffers->expert_output_scaling_factor,
                scaling_factor_sz, cudaMemcpyDeviceToDevice, args.stream));
        }
    }

    nvtxRangePop();  // End of dispatch_postprocess nvtx range
    return std::make_tuple(dispatched_tokens, dispatched_probs, dispatched_scaling_factor);
}

void Executor::combine_preprocess(HybridEpConfigInstance config, CombineArgs& args) {
    nvtxRangePushA("combine_preprocess in hybrid-ep");

    if(args.enable_unpermute) {
        // If enable_unpermute is true, unpermute the token/probs according to the
        // routing map.
        assert(args.row_id_map.has_value());
        assert(args.num_dispatched_tokens_tensor.has_value());
        auto num_dispatched_tokens_tensor = args.num_dispatched_tokens_tensor.value();
    
        UnpermuteArgs unpermute_args;
        unpermute_args.permuted_tokens = args.hidden;
        unpermute_args.permuted_probs = args.probs;
        unpermute_args.tokens_ptr = reinterpret_cast<uint16_t*>(intra_node_combine_buffers->expert_input_token);
        unpermute_args.probs_ptr = reinterpret_cast<float*>(intra_node_combine_buffers->expert_input_prob);
        unpermute_args.row_id_map = args.row_id_map.value();
        unpermute_args.num_of_local_experts = config.num_of_experts_per_rank;
        unpermute_args.num_dispatched_tokens_tensor = num_dispatched_tokens_tensor;
        unpermute_args.pad_multiple = args.pad_multiple;
        unpermute_args.hidden_size = config.hidden_dim;
        unpermute_args.local_rank = local_rank;
        unpermute_args.num_ranks_per_node = config.num_of_ranks_per_node;
        unpermute_args.with_probs = config.backward_combine_api;
        unpermute_args.stream = args.stream;
        unpermute_args.num_of_blocks_permute_api = config.num_of_blocks_permute_api;
        
        unpermute_launcher<uint16_t, float>(unpermute_args);
    
    }else{
        // Copy the input tensor to the input buffer
        auto input_sz = args.hidden.numel() * sizeof(uint16_t);
        CUDA_CHECK(
            cudaMemcpyAsync(intra_node_combine_buffers->expert_input_token,
                            reinterpret_cast<uint16_t *>(args.hidden.data_ptr()), input_sz,
                            cudaMemcpyDeviceToDevice, args.stream));
        if (config.backward_combine_api) {
            auto probs_sz = args.probs.numel() * sizeof(float);
            CUDA_CHECK(cudaMemcpyAsync(intra_node_combine_buffers->expert_input_prob,
                                       reinterpret_cast<float*>(args.probs.data_ptr()), probs_sz,
                                       cudaMemcpyDeviceToDevice, args.stream));
        }
    }

    nvtxRangePop();  // End of combine_preprocess nvtx range
}

void Executor::combine_core(HybridEpConfigInstance config, CombineArgs& args) {
    nvtxRangePushA("combine_core in hybrid-ep");
    hybrid_ep::combine_kernel_param_t param;
    
    // Setup input pointers
    for (int i = 0; i < config.num_of_ranks_per_node; i++) {
        param.expert_input_token[i] =
            intra_node_combine_buffers->expert_input_token_all_ranks[i];
        param.expert_input_prob[i] =
            intra_node_combine_buffers->expert_input_prob_all_ranks[i];
    }

    // Setup output pointers
    param.attn_output_token = reinterpret_cast<uint16_t*>(args.combined_tokens);
    param.attn_output_prob = (config.backward_combine_api) ? reinterpret_cast<float*>(args.combined_probs) : nullptr;

    // Setup local buffer pointers
#ifdef HYBRID_EP_BUILD_MULTINODE_ENABLE
    param.rdma_intra_node_red_token =
        inter_node_combine_buffers->rdma_intra_node_red_token;
    param.rdma_intra_node_red_prob = inter_node_combine_buffers->rdma_intra_node_red_prob;
    param.rdma_inter_node_group_token =
        inter_node_combine_buffers->rdma_inter_node_group_token;
    param.rdma_inter_node_group_prob =
        inter_node_combine_buffers->rdma_inter_node_group_prob;
    param.rdma_inter_node_group_flags =
        inter_node_combine_buffers->rdma_inter_node_group_flags;
#endif
    param.intra_node_write_completion_flags =
        intra_node_combine_buffers->intra_node_write_completion_flags;
    param.rdma_to_attn_map = args.rdma_to_attn_map.data_ptr<bool>();
    param.attn_to_rdma_map = args.attn_to_rdma_map.data_ptr<bool>();
    param.sparse_to_dense_map = args.sparse_to_dense_map.data_ptr<int32_t>();

    // Misc
    param.node_rank = this->node_rank;
    param.num_of_tokens_per_rank = args.num_of_tokens_per_rank;
    param.expected_intra_node_flag_value =
        intra_node_combine_buffers->expected_intra_node_flag_value;
#ifdef HYBRID_EP_BUILD_MULTINODE_ENABLE
    param.expected_rdma_flag_value = inter_node_combine_buffers->expected_rdma_flag_value;
    param.d_qps_gpu = reinterpret_cast<void **>(inter_node_combine_buffers->d_qps_gpu);
    param.mr_info = reinterpret_cast<void*>(inter_node_combine_buffers->mr_info);
#endif

    // Launch kernel
    kernel_cache.run_combine_kernel(config, param, args.stream);

    nvtxRangePop();  // End of combine_core nvtx range
}

void Executor::combine_postprocess(HybridEpConfigInstance config, CombineArgs& args) {
    nvtxRangePushA("combine_postprocess in hybrid-ep");
    // No postprocess is needed for the combine kernel now.
    nvtxRangePop();  // End of combine_postprocess nvtx range
}

